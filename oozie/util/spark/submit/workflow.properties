# Configures an oozie shell action to submit a Spark job via spark-submit.
# Even though OOZIE-1983 added support for a Spark action, we ran into a CLASSPATH
# problem when trying use Oozie with a Spark job that uses HiveContext.
# This spark-submit shell based workflow can be used to launch
# Spark jobs that require use of HiveContext.  If your Spark job doesn't use
# HiveContext, you should just use the built in Oozie Spark action.
#
# This workflow is meant to work with Spark applications that are able to take their
# CLI opts via a single flag that takes all options as a single string and then parses them.
# E.g. --options "-S true --input /path/to/input --output /path/to/output"
#
# You will probably be using this workflow.xml as a sub workflow.  However, if you would
# like to just submit a Spark job as a single workflow, you may use this
# workflow.properties file like:
#
#   oozie job -run -config oozie/util/spark/submit/workflow.properties \
#     -Dspark_app_name="My Awesome Spark App" \
#     -Dspark_app_jar=hdfs://analytics-hadoop/user/$USER/oozie/my_spark_app.jar \
#     -Dspark_app_class=org.wikimedia.analytics.refinery.job.MySparkJob \
#     -Dspark_app_options="..."
#

name_node                         = hdfs://analytics-hadoop
job_tracker                       = resourcemanager.analytics.eqiad.wmnet:8032
queue_name                        = default

# Base path in HDFS to refinery.
# When submitting this job for production, you should
# override this to point directly at a deployed
# directory name, and not the 'symbolic' 'current' directory.
# E.g.  /wmf/refinery/2015-01-05T17.59.18Z--7bb7f07
refinery_directory                = ${name_node}/wmf/refinery/current

# HDFS path to artifacts that will be used by this job.
# E.g. refinery-job.jar should exist here.
artifacts_directory               = ${refinery_directory}/artifacts

# Base path in HDFS to oozie files.
# Other files will be used relative to this path.
oozie_directory                   = ${refinery_directory}/oozie

# These are required properties.
spark_app_jar                    = ${artifacts_directory}/refinery-job.jar
#spark_app_class                  = org.wikimedia.analytics.job.MySparkApp
spark_app_name                    = spark-submit-${spark_app_class}

#spark_app_options                = --options "--my spark --job options"

# Optional properties.
#spark_app_name                   = spark-submit-${spark_app_class}
#queue_name                       = default
#hive_site_xml                    = ${name_node}/user/hive/hive-site.xml

# HDFS path to workflow to run.
workflow_file                     = ${oozie_directory}/util/spark/submit/workflow.xml

# Workflow app to run.
oozie.wf.application.path         = ${workflow_file}
oozie.use.system.libpath          = true
oozie.action.external.stats.write = true
